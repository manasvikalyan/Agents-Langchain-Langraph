{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "712b977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.documents import Document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05e61b",
   "metadata": {},
   "source": [
    "### chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "493b47ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.chat_models.base._ConfigurableModel object at 0x16c415640>\n"
     ]
    }
   ],
   "source": [
    "Groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "def load_model(model_name: str, api_key: str):\n",
    "    # model = ChatGroq(model_name=model_name, api_key=api_key)\n",
    "    # return model\n",
    "    model = init_chat_model(model_name=model_name, api_key=api_key)\n",
    "    return model\n",
    "\n",
    "llm = load_model(model_name=\"llama3-8b-8192\", api_key=Groq_api_key)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0538976",
   "metadata": {},
   "source": [
    "### embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd123ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010605454444885254, -0.08976001292467117, -0.014820730313658714]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "def embedding_model(model_name: str):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embedding_model(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "## test embedding model \n",
    "text = \"this is the document about embedding model\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(query_result[:3])\n",
    "\n",
    "embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c167c",
   "metadata": {},
   "source": [
    "### vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "69e8c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "# from langchain.schema import Document\n",
    "\n",
    "\n",
    "def create_vector_db(text_docs: list[str]):\n",
    "    # docs = [Document(page_content=t) for t in text_docs]\n",
    "\n",
    "    return Chroma(\n",
    "        collection_name=\"rag-collection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=\"chroma_db\"\n",
    "    )\n",
    "\n",
    "    # vector_store.add_documents(docs)\n",
    "    # return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b77948",
   "metadata": {},
   "source": [
    "Indexing\n",
    "1. Load: First we need to load our data. This is done with Document Loaders.\n",
    "2. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won’t fit in a model’s finite context window.\n",
    "3. Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "002d46c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_web_page() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# loader = WebBaseLoader(url, bs_kwargs={\"parse_only\": bs4_strainer})\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# docs = loader.load()\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# return docs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m docs = \u001b[43mload_web_page\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwww.google.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# print(docs[0].page_content[:2000])\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs))\n",
      "\u001b[31mTypeError\u001b[39m: load_web_page() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_web_page(url: str):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "    # bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "    # loader = WebBaseLoader(url, bs_kwargs={\"parse_only\": bs4_strainer})\n",
    "    # docs = loader.load()\n",
    "    # return docs\n",
    "\n",
    "docs = load_web_page(\"https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/\", \"www.google.com\")\n",
    "# print(docs[0].page_content[:2000])\n",
    "\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5dd28a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Developer skills\n",
      "\n",
      "Developer skillsResources for developers to grow in their skills and careers.\n",
      "\n",
      "Application developmentInsights and best practices for building apps.\n",
      "Career growthTips & tricks to grow as a professional developer.\n",
      "GitHubImprove how you use GitHub at work.\n",
      "GitHub EducationLearn how to move into your first professional role.\n",
      "Programming languages & frameworksStay current on what’s new (or new again).\n",
      "\n",
      "\n",
      "Get started with GitHub documentationLearn how to start building, shipping, and maintaining software with GitHub.Learn more\n",
      "\n",
      "\n",
      "Engineering\n",
      "\n",
      "EngineeringGet an inside look at how we’re building the home for all developers.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    \n",
    "# def split_text(docs: list[Document]):\n",
    "def split_text(docs: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200, add_start_index=True) \n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    return split_docs\n",
    "\n",
    "split_docs = split_text(docs)\n",
    "print(len(split_docs))\n",
    "print(split_docs[1].page_content[:3000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72023f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pydantic_core.core_schema import model_field\n",
    "def embedding_model(model_name: str, split_docs):\n",
    "    texts = [doc.page_content for doc in split_docs]\n",
    "    model_for_embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    vectors = model_for_embedding.embed_documents(texts)\n",
    "    return vectors, model_for_embedding\n",
    "\n",
    "embeddings, model_for_embedding = embedding_model(model_name=\"sentence-transformers/all-mpnet-base-v2\", split_docs=split_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "524c02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# Quick check of embedding dimensions for one chunk\n",
    "sample_embedding = model_for_embedding.embed_query(split_docs[0].page_content)\n",
    "print(len(sample_embedding))\n",
    "\n",
    "# print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "50f210d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "def create_vector_db(text_docs: list[Document], embeddings_model_obj):\n",
    "    return Chroma(\n",
    "        collection_name=\"agent-collection\",\n",
    "        embedding_function=embeddings_model_obj,  # embedding function , called here and while creation of vector store the embeddings are calculated\n",
    "        persist_directory=\"chroma_db\"\n",
    "    )\n",
    "\n",
    "vector_store = create_vector_db(split_docs, model_for_embedding)\n",
    " \n",
    "## the vector store will store text, metadata and embeddings\n",
    "doc_ids = vector_store.add_documents(split_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ad621de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: How to write a great agents.md: Lessons from over 2,500 repositories - The GitHub Blog\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tSkip to content\t\t\t\n",
      "\n",
      "\t\t\t\t\tSkip to sidebar\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/\n",
      "\n",
      "\tBlog\n",
      "\n",
      "Changelog\n",
      "Docs\n",
      "Customer stories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tTry GitHub Copilot\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tSee \n",
      "Embedding dim: 768\n",
      "First 5 dims: [ 0.01634688  0.03520707 -0.04967269 -0.02839436 -0.03164455]\n"
     ]
    }
   ],
   "source": [
    "record = vector_store.get(ids=[doc_ids[0]], include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "print(\"Text:\", record[\"documents\"][0][:300])\n",
    "print(\"Embedding dim:\", len(record[\"embeddings\"][0]))\n",
    "print(\"First 5 dims:\", record[\"embeddings\"][0][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3037ac",
   "metadata": {},
   "source": [
    "\n",
    "### RAG agents\n",
    "One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
