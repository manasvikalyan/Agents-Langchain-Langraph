{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3275d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import ScrapeWebsiteTool\n",
    "\n",
    "website_search_tool = ScrapeWebsiteTool(\n",
    "    website=\"https://aws.amazon.com/blogs/machine-learning/enabling-customers-to-deliver-production-ready-ai-agents-at-scale/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278c2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    " \n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "\n",
    "\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "import os\n",
    "from crewai import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"groq/llama-3.1-70b-versatile\",\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    is_litellm=True,  # force CrewAI to use LiteLLM (Groq provider)\n",
    ")\n",
    "\n",
    "\n",
    "# llm = LLM(model=\"groq/llama-3.1-70b-versatile\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2529dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "blog_research_agent = Agent(\n",
    "    role=\"Blog Research Agent\",\n",
    "    goal = \"Fetch all the information about the blog post from internet regarding the topic {topic}\",\n",
    "    backstory = \"Expert in finding information from the internet related to AI, genai, and other related topics\",\n",
    "    verbose = True,\n",
    "    memory = True,\n",
    "    tools = [],\n",
    "    allow_delegation = True,\n",
    "    llm = llm\n",
    "\n",
    ")\n",
    "\n",
    "summarizer_agent = Agent(\n",
    "    role = \"Blog Summarizer Agent\",\n",
    "    goal = \"Summarize the information about the blog post regarding the topic {topic}\",\n",
    "    backstory = \"Expert in summarizing blog posts keeping the important and relevant information. It should be concise and should explain the blog in points \",\n",
    "    verbose = True,\n",
    "    memory = True,\n",
    "    tools = [],\n",
    "    allow_delegation = False,\n",
    "    llm = llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a647377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_task = Task(\n",
    "    description = \"Identify the {topic} .Fetch all the information about the blog post from the internet\",\n",
    "    expected_output = \"Get detailed summary of the blog post\",\n",
    "    tools = [website_search_tool],\n",
    "    agent= blog_research_agent\n",
    ")\n",
    "\n",
    "summarize_task = Task(\n",
    "    description = \"Get the information about the blog and Summarize the information about the blog post regarding the topic {topic}\",\n",
    "    expected_output = \"A summary of the blog post in 5 to 10 points which should be details and has all the information about the topic\",\n",
    "    tools = [website_search_tool],\n",
    "    agent= summarizer_agent,\n",
    "    output_file= \"output.md\",\n",
    "    async_execution= False\n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71052397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error during short_term search: The OPENAI_API_KEY environment variable is not set.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/memory/storage/rag_storage.py\", line 173, in search\n",
      "    client = self._get_client()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/memory/storage/rag_storage.py\", line 103, in _get_client\n",
      "    return self._client if self._client else get_rag_client()\n",
      "                                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/rag/config/utils.py\", line 70, in get_rag_client\n",
      "    get_rag_config()\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/rag/config/utils.py\", line 50, in get_rag_config\n",
      "    default_config = config_class()\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n",
      "    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/rag/chromadb/config.py\", line 59, in _default_embedding_function\n",
      "    OpenAIEmbeddingFunction(\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/chromadb/utils/embedding_functions/openai_embedding_function.py\", line 63, in __init__\n",
      "    raise ValueError(f\"The {api_key_env_var} environment variable is not set.\")\n",
      "ValueError: The OPENAI_API_KEY environment variable is not set.\n",
      "\n",
      "ERROR:root:Error during entities search: The OPENAI_API_KEY environment variable is not set.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/memory/storage/rag_storage.py\", line 173, in search\n",
      "    client = self._get_client()\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/memory/storage/rag_storage.py\", line 103, in _get_client\n",
      "    return self._client if self._client else get_rag_client()\n",
      "                                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/rag/config/utils.py\", line 70, in get_rag_client\n",
      "    get_rag_config()\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/rag/config/utils.py\", line 50, in get_rag_config\n",
      "    default_config = config_class()\n",
      "                     ^^^^^^^^^^^^^^\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\n",
      "    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/rag/chromadb/config.py\", line 59, in _default_embedding_function\n",
      "    OpenAIEmbeddingFunction(\n",
      "  File \"/Users/manasvi/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/chromadb/utils/embedding_functions/openai_embedding_function.py\", line 63, in __init__\n",
      "    raise ValueError(f\"The {api_key_env_var} environment variable is not set.\")\n",
      "ValueError: The OPENAI_API_KEY environment variable is not set.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤– Agent Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Blog Research Agent</span>                                                                                     <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Task: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Identify the Blog on machine learning .Fetch all the information about the blog post from the internet</span>   <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35mâ•­â”€\u001b[0m\u001b[35mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[35m ğŸ¤– Agent Started \u001b[0m\u001b[35mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[35mâ”€â•®\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m                                                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mBlog Research Agent\u001b[0m                                                                                     \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m                                                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m  \u001b[37mTask: \u001b[0m\u001b[92mIdentify the Blog on machine learning .Fetch all the information about the blog post from the internet\u001b[0m   \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m                                                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Execution Traces</span><span style=\"color: #008080; text-decoration-color: #008080\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ” Detailed execution traces are available!</span>                                                                    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">View insights including:</span>                                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff\">  â€¢ Agent decision-making process</span>                                                                              <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff\">  â€¢ Task execution flow and timing</span>                                                                             <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #0000ff; text-decoration-color: #0000ff\">  â€¢ Tool usage details</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mExecution Traces\u001b[0m\u001b[36m \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[1;36mğŸ” \u001b[0m\u001b[1;36mDetailed execution traces are available!\u001b[0m                                                                    \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[37mView insights including:\u001b[0m                                                                                       \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[94m  â€¢ Agent decision-making process\u001b[0m                                                                              \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[94m  â€¢ Task execution flow and timing\u001b[0m                                                                             \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[94m  â€¢ Tool usage details\u001b[0m                                                                                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to view your execution traces? [y/N] (20s timeout): "
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: GroqException - {\"error\":{\"message\":\"property 'is_litellm' is unsupported\",\"type\":\"invalid_request_error\"}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:209\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     response = \u001b[43msync_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:944\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    943\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:926\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    925\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/main.py:1945\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   1943\u001b[39m             optional_params[k] = v\n\u001b[32m-> \u001b[39m\u001b[32m1945\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1949\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001b[39;49;00m\n\u001b[32m   1961\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33maiohttp_openai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1964\u001b[39m     \u001b[38;5;66;03m# NEW aiohttp provider for 10-100x higher RPS\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:512\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config, shared_session)\u001b[39m\n\u001b[32m    510\u001b[39m     sync_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    524\u001b[39m     model=model,\n\u001b[32m    525\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    534\u001b[39m     json_mode=json_mode,\n\u001b[32m    535\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:234\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:3587\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   3581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   3582\u001b[39m         status_code=status_code,\n\u001b[32m   3583\u001b[39m         message=error_text,\n\u001b[32m   3584\u001b[39m         headers=error_headers,\n\u001b[32m   3585\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3587\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   3588\u001b[39m     error_message=error_text,\n\u001b[32m   3589\u001b[39m     status_code=status_code,\n\u001b[32m   3590\u001b[39m     headers=error_headers,\n\u001b[32m   3591\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: {\"error\":{\"message\":\"property 'is_litellm' is unsupported\",\"type\":\"invalid_request_error\"}}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      6\u001b[39m crew = Crew(\n\u001b[32m      7\u001b[39m   agents=[blog_research_agent, summarizer_agent],\n\u001b[32m      8\u001b[39m   tasks=[research_task, summarize_task],\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m   share_crew=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m## start the task execution process with enhanced feedback\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result=\u001b[43mcrew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBlog on machine learning\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/crew.py:705\u001b[39m, in \u001b[36mCrew.kickoff\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    702\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_crew_planning()\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.sequential:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.hierarchical:\n\u001b[32m    707\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_hierarchical_process()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/crew.py:821\u001b[39m, in \u001b[36mCrew._run_sequential_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CrewOutput:\n\u001b[32m    820\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/crew.py:929\u001b[39m, in \u001b[36mCrew._execute_tasks\u001b[39m\u001b[34m(self, tasks, start_index, was_replayed)\u001b[39m\n\u001b[32m    926\u001b[39m     futures.clear()\n\u001b[32m    928\u001b[39m context = \u001b[38;5;28mself\u001b[39m._get_context(task, task_outputs)\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m task_output = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m task_outputs.append(task_output)\n\u001b[32m    935\u001b[39m \u001b[38;5;28mself\u001b[39m._process_task_result(task, task_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/task.py:458\u001b[39m, in \u001b[36mTask.execute_sync\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_sync\u001b[39m(\n\u001b[32m    452\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    453\u001b[39m     agent: BaseAgent | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    454\u001b[39m     context: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    455\u001b[39m     tools: \u001b[38;5;28mlist\u001b[39m[BaseTool] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    456\u001b[39m ) -> TaskOutput:\n\u001b[32m    457\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/task.py:590\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28mself\u001b[39m.end_time = datetime.datetime.now()\n\u001b[32m    589\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error=\u001b[38;5;28mstr\u001b[39m(e), task=\u001b[38;5;28mself\u001b[39m))  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/task.py:522\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28mself\u001b[39m.processed_by_agents.add(agent.role)\n\u001b[32m    521\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context=context, task=\u001b[38;5;28mself\u001b[39m))  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._guardrails \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._guardrail:\n\u001b[32m    529\u001b[39m     pydantic_output, json_output = \u001b[38;5;28mself\u001b[39m._export_output(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/agent/core.py:513\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m    505\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    506\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m         ),\n\u001b[32m    512\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    514\u001b[39m \u001b[38;5;28mself\u001b[39m._times_executed += \u001b[32m1\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._times_executed > \u001b[38;5;28mself\u001b[39m.max_retry_limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/agent/core.py:489\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    485\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._execute_with_timeout(\n\u001b[32m    486\u001b[39m             task_prompt, task, \u001b[38;5;28mself\u001b[39m.max_execution_time\n\u001b[32m    487\u001b[39m         )\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_without_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Propagate TimeoutError without retry\u001b[39;00m\n\u001b[32m    493\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    494\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    495\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m         ),\n\u001b[32m    500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/agent/core.py:591\u001b[39m, in \u001b[36mAgent._execute_without_timeout\u001b[39m\u001b[34m(self, task_prompt, task)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_executor:\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAgent executor is not initialized.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mask_for_human_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:180\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.ask_for_human_input = \u001b[38;5;28mbool\u001b[39m(inputs.get(\u001b[33m\"\u001b[39m\u001b[33mask_for_human_input\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     formatted_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    183\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    184\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    185\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:276\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_context_length_exceeded(e):\n\u001b[32m    278\u001b[39m         handle_context_length(\n\u001b[32m    279\u001b[39m             respect_context_window=\u001b[38;5;28mself\u001b[39m.respect_context_window,\n\u001b[32m    280\u001b[39m             printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m             i18n=\u001b[38;5;28mself\u001b[39m._i18n,\n\u001b[32m    285\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:220\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    209\u001b[39m     formatted_answer = handle_max_iterations_exceeded(\n\u001b[32m    210\u001b[39m         formatted_answer,\n\u001b[32m    211\u001b[39m         printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m    216\u001b[39m     )\n\u001b[32m    218\u001b[39m enforce_rpm_limit(\u001b[38;5;28mself\u001b[39m.request_within_rpm_limit)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m answer = \u001b[43mget_llm_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m formatted_answer = process_llm_response(answer, \u001b[38;5;28mself\u001b[39m.use_stop_words)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n\u001b[32m    232\u001b[39m     \u001b[38;5;66;03m# Extract agent fingerprint if available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:259\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer, from_task, from_agent, response_model)\u001b[39m\n\u001b[32m    251\u001b[39m     answer = llm.call(\n\u001b[32m    252\u001b[39m         messages,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    253\u001b[39m         callbacks=callbacks,\n\u001b[32m   (...)\u001b[39m\u001b[32m    256\u001b[39m         response_model=response_model,\n\u001b[32m    257\u001b[39m     )\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[32m    261\u001b[39m     printer.print(\n\u001b[32m    262\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mReceived None or empty response from LLM call.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:251\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer, from_task, from_agent, response_model)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\u001b[39;00m\n\u001b[32m    233\u001b[39m \n\u001b[32m    234\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \u001b[33;03m    ValueError: If the response is None or empty.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     answer = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/llm.py:1207\u001b[39m, in \u001b[36mLLM.call\u001b[39m\u001b[34m(self, messages, tools, callbacks, available_functions, from_task, from_agent, response_model)\u001b[39m\n\u001b[32m   1197\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1198\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_streaming_response(\n\u001b[32m   1199\u001b[39m             params=params,\n\u001b[32m   1200\u001b[39m             callbacks=callbacks,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m             response_model=response_model,\n\u001b[32m   1205\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_non_streaming_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mavailable_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavailable_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LLMContextLengthExceededError:\n\u001b[32m   1216\u001b[39m     \u001b[38;5;66;03m# Re-raise LLMContextLengthExceededError as it should be handled\u001b[39;00m\n\u001b[32m   1217\u001b[39m     \u001b[38;5;66;03m# by the CrewAgentExecutor._invoke_loop method, which can then decide\u001b[39;00m\n\u001b[32m   1218\u001b[39m     \u001b[38;5;66;03m# whether to summarize the content or abort based on the respect_context_window flag\u001b[39;00m\n\u001b[32m   1219\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/crewai/llm.py:967\u001b[39m, in \u001b[36mLLM._handle_non_streaming_response\u001b[39m\u001b[34m(self, params, callbacks, available_functions, from_task, from_agent, response_model)\u001b[39m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response_model:\n\u001b[32m    966\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33mresponse_model\u001b[39m\u001b[33m\"\u001b[39m] = response_model\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ContextWindowExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# Convert litellm's context window error to our own exception type\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;66;03m# for consistent handling in the rest of the codebase\u001b[39;00m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMContextLengthExceededError(\u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/utils.py:1381\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1378\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1379\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1380\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/utils.py:1250\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1248\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1249\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1253\u001b[39m     kwargs=kwargs,\n\u001b[32m   1254\u001b[39m     call_type=call_type,\n\u001b[32m   1255\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/main.py:3774\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3772\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3773\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3777\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3778\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3780\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2328\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/Agents-Langchain-Langraph/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:417\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    413\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minvalid_request_error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    414\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mIncorrect API key provided\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    415\u001b[39m ):\n\u001b[32m    416\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m    418\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    419\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    420\u001b[39m         model=model,\n\u001b[32m    421\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    422\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    423\u001b[39m         body=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    424\u001b[39m     )\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    426\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWeb server is returning an unknown error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    427\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe server had an error processing your request.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    428\u001b[39m ):\n\u001b[32m    429\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: GroqException - {\"error\":{\"message\":\"property 'is_litellm' is unsupported\",\"type\":\"invalid_request_error\"}}\n"
     ]
    }
   ],
   "source": [
    "from crewai import Crew,Process\n",
    "\n",
    "\n",
    "\n",
    "# Forming the tech-focused crew with some enhanced configurations\n",
    "crew = Crew(\n",
    "  agents=[blog_research_agent, summarizer_agent],\n",
    "  tasks=[research_task, summarize_task],\n",
    "  process=Process.sequential,  # Optional: Sequential task execution is default\n",
    "  memory=True,\n",
    "  cache=True,\n",
    "  max_rpm=100,\n",
    "  share_crew=True\n",
    ")\n",
    "\n",
    "## start the task execution process with enhanced feedback\n",
    "result=crew.kickoff(inputs={'topic':'Blog on machine learning'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92588846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
